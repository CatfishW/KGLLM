"""
RAG-Guided Diffusion Generator: Combines FAISS retrieval with diffusion generation.

This module provides two modes:
1. Generation Mode: Use retrieved paths as conditioning for diffusion generation
2. Reranking Mode: Score retrieved paths using diffusion likelihood

The key insight is that retrieval provides grounding (what paths are plausible)
while diffusion provides generation (producing diverse, coherent paths).
"""

import sys
import json
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
from dataclasses import dataclass, field

import numpy as np

# Add Core to path for diffusion imports
CORE_PATH = Path(__file__).parent.parent / "Core"
if str(CORE_PATH) not in sys.path:
    sys.path.insert(0, str(CORE_PATH))

import torch
import torch.nn.functional as F
from transformers import AutoTokenizer

from .config import RAGConfig, DEFAULT_CONFIG
from .retriever import KGPathRetriever, RetrievedPath


@dataclass
class GeneratedPath:
    """A path generated by the RAG-guided diffusion model."""
    relations: List[str]
    relation_chain: str
    score: float
    hop_count: int
    retrieval_conditioned: bool = True
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'relations': self.relations,
            'relation_chain': self.relation_chain,
            'score': self.score,
            'hop_count': self.hop_count,
            'retrieval_conditioned': self.retrieval_conditioned
        }


@dataclass
class GenerationResult:
    """Result from RAG-guided generation."""
    question: str
    generated_paths: List[GeneratedPath]
    retrieved_paths: List[RetrievedPath]
    predicted_hop_count: int
    generation_mode: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'question': self.question,
            'generated_paths': [p.to_dict() for p in self.generated_paths],
            'retrieved_paths': [p.to_dict() for p in self.retrieved_paths],
            'predicted_hop_count': self.predicted_hop_count,
            'generation_mode': self.generation_mode
        }
    
    def format_readable(self) -> str:
        lines = [
            f"Question: {self.question}",
            f"Predicted Hop Count: {self.predicted_hop_count}",
            f"Mode: {self.generation_mode}",
            "",
            "Generated Paths:"
        ]
        for i, path in enumerate(self.generated_paths, 1):
            lines.append(f"  [{i}] {path.relation_chain} (score: {path.score:.4f})")
        
        lines.append("")
        lines.append("Retrieved Candidates (used for conditioning):")
        for path in self.retrieved_paths[:3]:
            lines.append(f"  - {path.relation_chain} (score: {path.score:.4f})")
        
        return "\n".join(lines)


class RAGGuidedDiffusionGenerator:
    """
    RAG-Guided Diffusion Generator for Knowledge Graph Path Generation.
    
    Combines FAISS-based retrieval with diffusion model generation to produce
    diverse, grounded relation paths.
    
    Two modes:
    1. Generation: Retrieve candidates -> Condition diffusion -> Generate paths
    2. Reranking: Retrieve candidates -> Score with diffusion -> Rerank
    
    Example:
        >>> generator = RAGGuidedDiffusionGenerator(
        ...     model_path="Core/diffusion_causal_hopcount_20251215/checkpoints/best.ckpt",
        ...     index_path="EXP/index"
        ... )
        >>> result = generator.generate("who is obama's wife", num_paths=5)
        >>> print(result.format_readable())
    """
    
    def __init__(
        self,
        model_path: Optional[str] = None,
        index_path: str = "EXP/index",
        vocab_path: str = "Data/vocab_combined.json",
        config: Optional[RAGConfig] = None,
        device: Optional[str] = None,
        retrieval_top_k: int = 10,
        conditioning_weight: float = 0.3,
        diversity_penalty: float = 0.5
    ):
        """
        Initialize RAG-Guided Diffusion Generator.
        
        Args:
            model_path: Path to trained diffusion model checkpoint
            index_path: Path to FAISS index directory
            vocab_path: Path to vocabulary JSON file
            config: RAG configuration
            device: Device to use (cuda/cpu)
            retrieval_top_k: Number of paths to retrieve for conditioning
            conditioning_weight: Weight for retrieval conditioning (0-1)
            diversity_penalty: Penalty for repeating relations
        """
        self.config = config or DEFAULT_CONFIG
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.retrieval_top_k = retrieval_top_k
        self.conditioning_weight = conditioning_weight
        self.diversity_penalty = diversity_penalty
        
        # Lazy loaded components
        self._retriever: Optional[KGPathRetriever] = None
        self._model = None
        self._tokenizer = None
        self._vocab = None
        
        self.model_path = model_path
        self.index_path = Path(index_path)
        self.vocab_path = Path(vocab_path)
        
        # Load vocab early (needed for relation decoding)
        self._load_vocab()
    
    def _load_vocab(self):
        """Load relation vocabulary."""
        try:
            if self.vocab_path.exists():
                with open(self.vocab_path, 'r') as f:
                    self._vocab = json.load(f)
                self.relation2idx = self._vocab.get('relation2idx', {})
                self.idx2relation = {v: k for k, v in self.relation2idx.items()}
                print(f"Loaded vocabulary with {len(self.relation2idx)} relations")
            else:
                print(f"Vocabulary not found at {self.vocab_path}")
                self.relation2idx = {}
                self.idx2relation = {}
        except Exception as e:
            print(f"Failed to load vocabulary: {e}")
            self.relation2idx = {}
            self.idx2relation = {}
    
    @property
    def retriever(self) -> KGPathRetriever:
        """Lazy load retriever."""
        if self._retriever is None:
            self._retriever = KGPathRetriever(str(self.index_path), self.config)
        return self._retriever
    
    @property
    def model(self):
        """Lazy load diffusion model."""
        if self._model is None and self.model_path:
            self._load_model()
        return self._model
    
    @property
    def tokenizer(self):
        """Lazy load tokenizer."""
        if self._tokenizer is None:
            self._tokenizer = AutoTokenizer.from_pretrained(
                "sentence-transformers/all-MiniLM-L6-v2"
            )
        return self._tokenizer
    
    def _load_model(self):
        """Load diffusion model from checkpoint."""
        if not self.model_path:
            print("No model path specified")
            return
        
        checkpoint_path = Path(self.model_path)
        if not checkpoint_path.exists():
            print(f"Model checkpoint not found: {self.model_path}")
            return
        
        try:
            # Import the Lightning module
            from kg_path_diffusion import KGPathDiffusionLightning
            
            # Load checkpoint
            self._model = KGPathDiffusionLightning.load_from_checkpoint(
                str(checkpoint_path),
                strict=False,
                map_location=self.device
            )
            self._model.eval()
            self._model.to(self.device)
            
            print(f"Loaded diffusion model from {self.model_path}")
            
            # Check for hop predictor
            if hasattr(self._model.model, 'hop_predictor') and self._model.model.hop_predictor is not None:
                print("  - Hop predictor: ENABLED")
            else:
                print("  - Hop predictor: DISABLED")
            
            # Check for causal attention
            if hasattr(self._model.model, 'use_causal_attention'):
                print(f"  - Causal attention: {self._model.model.use_causal_attention}")
                
        except Exception as e:
            print(f"Failed to load model: {e}")
            import traceback
            traceback.print_exc()
            self._model = None
    
    def _encode_question(self, question: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """Encode question for model input."""
        inputs = self.tokenizer(
            question,
            return_tensors='pt',
            padding='max_length',
            max_length=128,
            truncation=True
        )
        return (
            inputs['input_ids'].to(self.device),
            inputs['attention_mask'].to(self.device)
        )
    
    def _predict_hop_count(self, question: str) -> int:
        """Predict the number of hops needed for this question."""
        if self.model is None or not hasattr(self.model.model, 'hop_predictor'):
            return 2  # Default fallback
        
        if self.model.model.hop_predictor is None:
            return 2
        
        input_ids, attention_mask = self._encode_question(question)
        
        with torch.no_grad():
            question_seq, question_pooled = self.model.model.encode_inputs(
                input_ids, attention_mask
            )
            hop_count = self.model.model.hop_predictor.predict_hop_count(question_pooled)
            return hop_count.item()
    
    def _get_retrieval_bias(
        self,
        retrieved_paths: List[RetrievedPath]
    ) -> Optional[torch.Tensor]:
        """
        Create a bias tensor from retrieved paths to guide generation.
        
        Args:
            retrieved_paths: List of retrieved paths
            
        Returns:
            Tensor of shape [num_relations] with bias scores
        """
        if not retrieved_paths or not self.relation2idx:
            return None
        
        # Count relation frequencies in retrieved paths
        relation_counts = {}
        total_weight = 0.0
        
        for path in retrieved_paths:
            weight = path.score
            total_weight += weight
            for relation in path.relations:
                if relation in self.relation2idx:
                    idx = self.relation2idx[relation]
                    relation_counts[idx] = relation_counts.get(idx, 0) + weight
        
        if not relation_counts or total_weight == 0:
            return None
        
        # Create bias tensor
        num_relations = len(self.relation2idx) + 10  # +10 for special tokens
        bias = torch.zeros(num_relations, device=self.device)
        
        for idx, count in relation_counts.items():
            bias[idx] = count / total_weight
        
        # Normalize and scale by conditioning weight
        bias = bias * self.conditioning_weight
        
        return bias
    
    def _decode_relations(self, relation_ids: torch.Tensor) -> List[str]:
        """Convert relation indices to strings."""
        relations = []
        for idx in relation_ids.cpu().tolist():
            if idx in self.idx2relation:
                rel = self.idx2relation[idx]
                # Skip special tokens
                if rel not in ['<PAD>', '<BOS>', '<MASK>', '<EOS>', '<UNK>']:
                    relations.append(rel)
            elif idx > 4:  # Skip special token indices
                relations.append(f"relation_{idx}")
        return relations
    
    def _apply_diversity_penalty(
        self,
        logits: torch.Tensor,
        generated_relations: List[List[int]],
        penalty: float
    ) -> torch.Tensor:
        """Apply penalty for previously generated relations."""
        if not generated_relations or penalty == 0:
            return logits
        
        # Count relations in previously generated paths
        relation_counts = {}
        for path in generated_relations:
            for rel_idx in path:
                relation_counts[rel_idx] = relation_counts.get(rel_idx, 0) + 1
        
        # Apply penalty
        for rel_idx, count in relation_counts.items():
            if rel_idx < logits.shape[-1]:
                logits[..., rel_idx] -= penalty * count
        
        return logits
    
    def generate(
        self,
        question: str,
        num_paths: int = 5,
        use_retrieval_conditioning: bool = True,
        temperature: float = 0.8,
        mode: str = "generation"
    ) -> GenerationResult:
        """
        Generate relation paths for a question.
        
        Args:
            question: Natural language question
            num_paths: Number of paths to generate
            use_retrieval_conditioning: Whether to condition on retrieved paths
            temperature: Sampling temperature
            mode: "generation" or "reranking"
            
        Returns:
            GenerationResult with generated and retrieved paths
        """
        # Step 1: Retrieve candidate paths
        retrieved_paths = self.retriever.retrieve(
            question, 
            top_k=self.retrieval_top_k
        )
        
        # Step 2: Predict hop count
        predicted_hops = self._predict_hop_count(question)
        path_length = predicted_hops + 1  # +1 for entity count
        
        # Step 3: Generate or rerank based on mode
        if mode == "reranking":
            generated = self._rerank_paths(question, retrieved_paths)
        else:
            generated = self._generate_paths(
                question,
                retrieved_paths if use_retrieval_conditioning else [],
                num_paths,
                path_length,
                temperature
            )
        
        return GenerationResult(
            question=question,
            generated_paths=generated,
            retrieved_paths=retrieved_paths,
            predicted_hop_count=predicted_hops,
            generation_mode=mode
        )
    
    def _generate_paths(
        self,
        question: str,
        retrieved_paths: List[RetrievedPath],
        num_paths: int,
        path_length: int,
        temperature: float
    ) -> List[GeneratedPath]:
        """Generate paths using diffusion model."""
        if self.model is None:
            print("Model not loaded, falling back to retrieved paths")
            return [
                GeneratedPath(
                    relations=p.relations,
                    relation_chain=p.relation_chain,
                    score=p.score,
                    hop_count=len(p.relations),
                    retrieval_conditioned=False
                )
                for p in retrieved_paths[:num_paths]
            ]
        
        generated_paths = []
        generated_relation_ids = []  # Track for diversity
        
        # Get retrieval bias
        retrieval_bias = self._get_retrieval_bias(retrieved_paths)
        
        # Encode question once
        input_ids, attention_mask = self._encode_question(question)
        
        with torch.no_grad():
            question_seq, question_pooled = self.model.model.encode_inputs(
                input_ids, attention_mask
            )
            question_mask = ~attention_mask.bool()
            
            for path_idx in range(num_paths):
                # Adjust temperature for diversity
                curr_temp = temperature * (1 + self.diversity_penalty * path_idx * 0.1)
                
                # Generate using diffusion
                entities, relations = self._generate_single_path(
                    question_seq,
                    question_mask,
                    path_length,
                    curr_temp,
                    retrieval_bias,
                    generated_relation_ids
                )
                
                # Decode relations
                relation_strs = self._decode_relations(relations[0])
                
                if relation_strs:
                    generated_relation_ids.append(relations[0].cpu().tolist())
                    
                    # Compute score based on uniqueness and retrieval alignment
                    score = self._compute_path_score(
                        relation_strs, 
                        retrieved_paths,
                        generated_paths
                    )
                    
                    generated_paths.append(GeneratedPath(
                        relations=relation_strs,
                        relation_chain=" -> ".join(relation_strs),
                        score=score,
                        hop_count=len(relation_strs),
                        retrieval_conditioned=bool(retrieved_paths)
                    ))
        
        # Sort by score
        generated_paths.sort(key=lambda x: -x.score)
        
        return generated_paths
    
    def _generate_single_path(
        self,
        question_seq: torch.Tensor,
        question_mask: torch.Tensor,
        path_length: int,
        temperature: float,
        retrieval_bias: Optional[torch.Tensor],
        generated_relation_ids: List[List[int]]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Generate a single path using the diffusion model."""
        B = question_seq.shape[0]
        
        # Start with masked sequence
        relation_mask_idx = 2  # Typically MASK token
        relations = torch.full(
            (B, path_length - 1), 
            relation_mask_idx, 
            device=self.device,
            dtype=torch.long
        )
        
        # Dummy entities (not predicting in relation-only mode)
        entities = torch.zeros((B, path_length), device=self.device, dtype=torch.long)
        
        # Iteratively denoise
        diffusion = self.model.model.diffusion
        denoiser = self.model.model.denoiser
        
        for t in reversed(range(diffusion.num_timesteps)):
            t_batch = torch.full((B,), t, device=self.device, dtype=torch.long)
            
            # Get predictions
            entity_logits, relation_logits = denoiser(
                None,  # No entity input in relation-only mode
                relations,
                t_batch,
                question_seq,
                question_mask=question_mask
            )
            
            # Apply retrieval bias
            if retrieval_bias is not None and relation_logits is not None:
                # Expand bias to match logits shape
                bias_expanded = retrieval_bias.unsqueeze(0).unsqueeze(0)
                bias_expanded = bias_expanded.expand(
                    relation_logits.shape[0],
                    relation_logits.shape[1],
                    -1
                )
                # Ensure shapes match
                if bias_expanded.shape[-1] <= relation_logits.shape[-1]:
                    relation_logits[..., :bias_expanded.shape[-1]] += bias_expanded
            
            # Apply diversity penalty
            relation_logits = self._apply_diversity_penalty(
                relation_logits,
                generated_relation_ids,
                self.diversity_penalty
            )
            
            # Sample
            if temperature > 0 and relation_logits is not None:
                relation_logits = torch.clamp(relation_logits, min=-50.0, max=50.0)
                relation_probs = F.softmax(relation_logits / temperature, dim=-1)
                
                # Handle NaN
                if torch.isnan(relation_probs).any():
                    relation_probs = torch.ones_like(relation_probs) / relation_logits.shape[-1]
                
                relation_probs = torch.clamp(relation_probs, min=1e-8)
                relation_probs = relation_probs / relation_probs.sum(dim=-1, keepdim=True)
                
                relation_samples = torch.multinomial(
                    relation_probs.view(-1, relation_logits.shape[-1]), 1
                ).view(B, path_length - 1)
            else:
                relation_samples = relation_logits.argmax(dim=-1) if relation_logits is not None else relations
            
            # Update masked positions
            if t > 0:
                alpha_t = diffusion.alpha_cumprod[t]
                alpha_t_prev = diffusion.alpha_cumprod[t-1]
                unmask_prob = 1 - alpha_t_prev / alpha_t
                
                relation_unmask = torch.rand_like(relations.float()) < unmask_prob
                relations = torch.where(
                    relation_unmask & (relations == relation_mask_idx),
                    relation_samples,
                    relations
                )
            else:
                relations = relation_samples
        
        return entities, relations
    
    def _compute_path_score(
        self,
        relations: List[str],
        retrieved_paths: List[RetrievedPath],
        generated_paths: List[GeneratedPath]
    ) -> float:
        """Compute score for a generated path."""
        score = 1.0
        
        # Bonus for matching retrieved paths
        for rp in retrieved_paths:
            overlap = len(set(relations) & set(rp.relations))
            if overlap > 0:
                score += 0.1 * overlap * rp.score
        
        # Penalty for duplicating already generated paths
        for gp in generated_paths:
            if relations == gp.relations:
                score -= 0.5
            elif set(relations) == set(gp.relations):
                score -= 0.2
        
        # Penalty for repetitive relations within path
        unique_ratio = len(set(relations)) / len(relations) if relations else 0
        score *= unique_ratio
        
        return max(0.1, score)
    
    def _rerank_paths(
        self,
        question: str,
        retrieved_paths: List[RetrievedPath]
    ) -> List[GeneratedPath]:
        """Rerank retrieved paths using diffusion likelihood."""
        if self.model is None:
            # Fall back to retrieval scores
            return [
                GeneratedPath(
                    relations=p.relations,
                    relation_chain=p.relation_chain,
                    score=p.score,
                    hop_count=len(p.relations),
                    retrieval_conditioned=False
                )
                for p in retrieved_paths
            ]
        
        scored_paths = []
        input_ids, attention_mask = self._encode_question(question)
        
        with torch.no_grad():
            for path in retrieved_paths:
                # Encode path relations
                relation_ids = [
                    self.relation2idx.get(r, 1)  # 1 = UNK
                    for r in path.relations
                ]
                
                if not relation_ids:
                    continue
                
                relation_tensor = torch.tensor(
                    [relation_ids], 
                    dtype=torch.long, 
                    device=self.device
                )
                
                # Compute diffusion loss (lower = better fit)
                try:
                    outputs = self.model.model(
                        question_input_ids=input_ids,
                        question_attention_mask=attention_mask,
                        target_entities=torch.zeros_like(relation_tensor),
                        target_relations=relation_tensor
                    )
                    
                    loss = outputs['relation_loss'].item()
                    if not np.isnan(loss) and not np.isinf(loss):
                        # Convert loss to score (negative because lower loss = higher score)
                        diffusion_score = -loss
                    else:
                        diffusion_score = -10.0
                        
                except Exception as e:
                    diffusion_score = -10.0
                
                # Combine with retrieval score
                combined_score = (
                    (1 - self.conditioning_weight) * path.score +
                    self.conditioning_weight * (diffusion_score + 10) / 10  # Normalize to ~0-1
                )
                
                scored_paths.append(GeneratedPath(
                    relations=path.relations,
                    relation_chain=path.relation_chain,
                    score=combined_score,
                    hop_count=len(path.relations),
                    retrieval_conditioned=True
                ))
        
        # Sort by combined score
        scored_paths.sort(key=lambda x: -x.score)
        
        return scored_paths
    
    def is_ready(self) -> bool:
        """Check if generator is ready."""
        return self.model is not None and self._retriever is not None


def main():
    """Test the RAG-guided generator."""
    import argparse
    
    parser = argparse.ArgumentParser(description="RAG-Guided Diffusion Generator")
    parser.add_argument("--model-path", type=str, 
                        default="Core/diffusion_causal_hopcount_20251215/checkpoints/last.ckpt",
                        help="Path to diffusion model checkpoint")
    parser.add_argument("--index-path", type=str, default="EXP/index",
                        help="Path to FAISS index")
    parser.add_argument("--vocab-path", type=str, default="Data/vocab_combined.json",
                        help="Path to vocabulary")
    parser.add_argument("--question", type=str, 
                        default="what time zone am i in cleveland ohio",
                        help="Question to test")
    parser.add_argument("--num-paths", type=int, default=5,
                        help="Number of paths to generate")
    parser.add_argument("--mode", type=str, choices=["generation", "reranking"],
                        default="generation", help="Operating mode")
    parser.add_argument("--test", action="store_true", help="Run minimal test")
    
    args = parser.parse_args()
    
    if args.test:
        print("Testing RAG-Guided Diffusion Generator...")
        print("Import test: OK")
        return
    
    # Initialize generator
    generator = RAGGuidedDiffusionGenerator(
        model_path=args.model_path,
        index_path=args.index_path,
        vocab_path=args.vocab_path
    )
    
    # Generate paths
    print(f"\nQuestion: {args.question}")
    print(f"Mode: {args.mode}")
    print("-" * 60)
    
    result = generator.generate(
        args.question,
        num_paths=args.num_paths,
        mode=args.mode
    )
    
    print(result.format_readable())


if __name__ == "__main__":
    main()
