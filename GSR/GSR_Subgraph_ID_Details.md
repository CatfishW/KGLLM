# GSR Subgraph ID: Detailed Explanation with Concrete Examples

## Overview

In GSR, a **subgraph ID** is a unique identifier that represents a specific reasoning path pattern or subgraph structure in the knowledge graph. The model learns to generate these IDs, which are then used to retrieve the corresponding subgraph containing the answer.

---

## How Subgraph IDs Work

### 1. Subgraph ID Generation Process

The subgraph ID is generated by the GSR model (T5) in a text-to-text format:

```
Input: "Question: Who discovered penicillin?"
Output: "path_1_2_3"  or  "r1_r2_r3"  or  "people.person.profession|award.award_winner.awards_won"
```

The exact format depends on how the subgraph index is constructed, but typically it encodes:
- **Relation sequence**: The path of relations needed to answer the question
- **Subgraph pattern**: A specific subgraph structure pattern

### 2. Subgraph Index Structure

The subgraph index is built during preprocessing (`preprocess/get_example_examples.py`) and maps IDs to actual subgraph structures:

```python
# Subgraph Index (simplified structure)
subgraph_index = {
    "path_1_2_3": {
        "relations": ["people.person.profession", "award.award_winner.awards_won"],
        "entities": ["m.0abc", "m.0def", "m.0ghi"],
        "triples": [
            ("m.0abc", "people.person.profession", "m.0def"),
            ("m.0def", "award.award_winner.awards_won", "m.0ghi")
        ],
        "answer_entities": ["m.0ghi"]
    },
    "path_4_5": {
        "relations": ["people.person.place_of_birth"],
        "entities": ["m.0xyz", "m.0uvw"],
        "triples": [
            ("m.0xyz", "people.person.place_of_birth", "m.0uvw")
        ],
        "answer_entities": ["m.0uvw"]
    }
}
```

---

## Concrete Examples

### Example 1: Simple 1-hop Question

**Question**: "What is the capital of France?"

**GSR Model Output**:
```
subgraph_id: "path_location_capital"
```

**Retrieved Subgraph** (from index lookup):
```python
{
    "subgraph_id": "path_location_capital",
    "relations": ["location.location.capital"],
    "entities": ["m.0france", "m.0paris"],
    "triples": [
        ("m.0france", "location.location.capital", "m.0paris")
    ],
    "answer_entities": ["m.0paris"],
    "answer_text": "Paris"
}
```

**Reader Model Input**:
- Question: "What is the capital of France?"
- Retrieved Subgraph: The triples and entities above
- Output: "Paris"

---

### Example 2: 2-hop Question

**Question**: "Who discovered penicillin?"

**GSR Model Output**:
```
subgraph_id: "path_discovery_1"
```

**Retrieved Subgraph**:
```python
{
    "subgraph_id": "path_discovery_1",
    "relations": [
        "medicine.drug.discoverer",
        "people.person.name"
    ],
    "entities": ["m.0penicillin", "m.0fleming", "m.0alexander_fleming"],
    "triples": [
        ("m.0penicillin", "medicine.drug.discoverer", "m.0fleming"),
        ("m.0fleming", "people.person.name", "m.0alexander_fleming")
    ],
    "answer_entities": ["m.0fleming"],
    "answer_text": "Alexander Fleming"
}
```

**Reader Model** uses this subgraph to generate: "Alexander Fleming discovered penicillin."

---

### Example 3: Complex Multi-hop Question

**Question**: "What character did Natalie Portman play in Star Wars?"

**GSR Model Output**:
```
subgraph_id: "path_film_character_actor"
```

**Retrieved Subgraph**:
```python
{
    "subgraph_id": "path_film_character_actor",
    "relations": [
        "film.actor.film",
        "film.film.character",
        "film.performance.actor"
    ],
    "entities": [
        "m.0natalie_portman",
        "m.0star_wars",
        "m.0padme_amidala",
        "m.0performance_1"
    ],
    "triples": [
        ("m.0natalie_portman", "film.actor.film", "m.0star_wars"),
        ("m.0star_wars", "film.film.character", "m.0padme_amidala"),
        ("m.0performance_1", "film.performance.actor", "m.0natalie_portman"),
        ("m.0performance_1", "film.performance.character", "m.0padme_amidala")
    ],
    "answer_entities": ["m.0padme_amidala"],
    "answer_text": "Padmé Amidala"
}
```

---

## Subgraph ID Formats (Possible Implementations)

Based on the GSR paper and typical implementations, subgraph IDs could be encoded in several ways:

### Format 1: Relation Sequence Encoding
```
"r1_r2_r3"  →  ["people.person.sibling_s", "people.person.name"]
```

### Format 2: Path Pattern Hash
```
"path_a3f2b1"  →  Hash of relation sequence pattern
```

### Format 3: Human-Readable Pattern
```
"person_sibling_name"  →  Pattern: person → sibling → name
```

### Format 4: Numeric ID with Metadata
```
"subgraph_12345"  →  Index entry #12345 containing full subgraph
```

---

## Training Data Format

### GSR Training Sample:
```json
{
    "question": "What is the name of justin bieber brother?",
    "subgraph_id": "path_sibling_name",
    "answer": "Jaxon Bieber"
}
```

### Model Training Input/Output:
```
Input (to T5 encoder): "Question: What is the name of justin bieber brother?"
Target (from T5 decoder): "path_sibling_name"
```

### During Inference:
1. Model generates: `"path_sibling_name"`
2. System looks up index: `subgraph_index["path_sibling_name"]`
3. Retrieves subgraph with triples:
   ```python
   [
       ("m.0justin_bieber", "people.person.sibling_s", "m.0jaxon_bieber"),
       ("m.0jaxon_bieber", "people.person.name", "Jaxon Bieber")
   ]
   ```
4. Reader model uses this subgraph to generate final answer

---

## Subgraph Index Construction

The subgraph index is built by analyzing answer paths in the training data:

### Step 1: Extract Answer Paths
For each question-answer pair, extract the reasoning path:

```python
# Question: "What is the name of justin bieber brother?"
# Answer: "Jaxon Bieber"

# Extracted path:
path = {
    "entities": ["m.0justin_bieber", "m.0jaxon_bieber"],
    "relations": ["people.person.sibling_s"],
    "answer_entity": "m.0jaxon_bieber"
}
```

### Step 2: Create Subgraph Pattern
Identify the relation pattern:

```python
pattern = "people.person.sibling_s"
subgraph_id = f"path_{pattern.replace('.', '_')}"
# Result: "path_people_person_sibling_s"
```

### Step 3: Build Index Entry
```python
subgraph_index[subgraph_id] = {
    "relations": ["people.person.sibling_s"],
    "pattern": "person → sibling → name",
    "example_triples": [
        ("m.0justin_bieber", "people.person.sibling_s", "m.0jaxon_bieber")
    ],
    "answer_type": "person_name"
}
```

### Step 4: Generalize Pattern
Multiple questions might use the same pattern:
- "What is the name of X's brother?" → same pattern
- "What is the name of X's sister?" → same pattern (sibling_s)

So `path_people_person_sibling_s` can answer many questions with this pattern.

---

## Comparison with Our Approach

### GSR Approach:
```python
# Step 1: Generate subgraph ID
subgraph_id = model.generate("Question: Who is Justin Bieber's brother?")
# Output: "path_sibling_name"

# Step 2: Retrieve subgraph
subgraph = index[subgraph_id]
# Returns: Pre-defined subgraph structure

# Step 3: Reader generates answer
answer = reader_model(question, subgraph)
```

### Our Approach:
```python
# Direct path generation
path = model.generate(question, graph)
# Output: {
#     "entities": ["m.0justin_bieber", "m.0jaxon_bieber"],
#     "relations": ["people.person.sibling_s"]
# }

# Direct answer extraction
answer = extract_answer(path, graph)
```

---

## Advantages of Subgraph ID Approach

1. **Efficiency**: Only need to generate a short ID string instead of full path
2. **Generalization**: Same ID can work for multiple questions with same pattern
3. **Indexing**: Pre-computed subgraphs can be optimized for retrieval
4. **Modularity**: Separates retrieval (GSR) from answer generation (Reader)

## Limitations

1. **Pre-computation**: Must build and maintain subgraph index
2. **Coverage**: Can only retrieve pre-indexed subgraphs
3. **Flexibility**: Less flexible than direct path generation
4. **Index Size**: Large knowledge graphs require large indices

---

## Example: Complete Workflow

### Question: "Where was Barack Obama born?"

**Step 1: GSR Model (Retriever)**
```
Input: "Question: Where was Barack Obama born?"
Output: "path_birth_location"
```

**Step 2: Subgraph Retrieval**
```python
subgraph = index["path_birth_location"]
# Returns:
{
    "relations": ["people.person.place_of_birth"],
    "entities": ["m.0barack_obama", "m.0honolulu"],
    "triples": [
        ("m.0barack_obama", "people.person.place_of_birth", "m.0honolulu")
    ],
    "answer_entities": ["m.0honolulu"]
}
```

**Step 3: Reader Model**
```
Input: 
  - Question: "Where was Barack Obama born?"
  - Subgraph: [triples above]
  
Output: "Honolulu, Hawaii"
```

---

## Key Insight

The subgraph ID is essentially a **compressed representation** of a reasoning pattern. Instead of generating the full path sequence (like our approach), GSR generates a pattern identifier that maps to a pre-computed subgraph structure. This makes the model smaller and faster, but requires the indexing step and limits flexibility to pre-computed patterns.

---

## Implementation Details from GSR Repository

Based on the GSR repository structure and README:

### 1. Subgraph ID Generation Script
```bash
python gsr/inference.py \
    --model_path <path to trained GSR model> \
    --tokenizer_path <augmented T5 tokenizer path> \
    --output_dir <output directory> \
    --nums_beam <number of beams>
```

This script generates subgraph IDs for test questions. The output format is likely:
```json
{
    "question_id": "WebQTest-0",
    "question": "What does jamaican people speak?",
    "predicted_subgraph_ids": ["path_1", "path_2", "path_3"]  // Top-k predictions
}
```

### 2. Subgraph Index Construction

The index is built from Freebase SPARQL queries (`preprocess/get_example_examples.py`):

```python
# Pseudo-code for index construction
for question, answer in training_data:
    # Query Freebase for paths connecting question entity to answer
    paths = query_freebase_sparql(question_entity, answer_entity)
    
    # Extract relation patterns
    for path in paths:
        relation_sequence = extract_relations(path)
        subgraph_id = create_id_from_relations(relation_sequence)
        
        # Store in index
        subgraph_index[subgraph_id] = {
            "relations": relation_sequence,
            "entities": path.entities,
            "triples": path.triples,
            "answer_entities": [answer_entity]
        }
```

### 3. Actual Subgraph ID Format (Likely)

Based on the paper's focus on "subgraph retrieval" and the use of T5, the subgraph ID is likely a **relation sequence** encoded as text:

**Example 1:**
```
Question: "What is the capital of France?"
Generated ID: "location.location.capital"
Retrieved: Subgraph with relation "location.location.capital"
```

**Example 2:**
```
Question: "Who discovered penicillin?"
Generated ID: "medicine.drug.discoverer|people.person.name"
Retrieved: Subgraph with relations ["medicine.drug.discoverer", "people.person.name"]
```

**Example 3:**
```
Question: "What character did Natalie Portman play in Star Wars?"
Generated ID: "film.actor.film|film.film.character"
Retrieved: Subgraph connecting actor → film → character
```

### 4. Evaluation Process

The evaluation script (`gsr/inference.py --eval_only`) checks if the predicted subgraph IDs retrieve subgraphs that contain the answer:

```python
# Pseudo-code for evaluation
for question, answer in test_data:
    predicted_ids = model.generate(question)  # ["path_1", "path_2", ...]
    
    for subgraph_id in predicted_ids[:top_n]:
        subgraph = index[subgraph_id]
        
        # Check if answer entity is in retrieved subgraph
        if answer_entity in subgraph["answer_entities"]:
            hit = True
            break
    
    if hit:
        recall += 1
```

### 5. Reader Model Training

After GSR inference, reader training data is prepared (`preprocess/prepare_reader_data.py`):

```python
# Reader training sample
{
    "question": "What is the capital of France?",
    "subgraph": {
        "triples": [
            ("m.0france", "location.location.capital", "m.0paris")
        ],
        "entities": ["m.0france", "m.0paris"]
    },
    "answer": "Paris"
}
```

The reader model (typically an LLM fine-tuned with QLoRA) learns to generate answers from the retrieved subgraph.

---

## Concrete Example: Full Pipeline

### Question: "What is the name of Justin Bieber's brother?"

**Step 1: GSR Model Inference**
```python
input_text = "Question: What is the name of Justin Bieber's brother?"
output = t5_model.generate(input_text, max_length=50)
# Output: "people.person.sibling_s|people.person.name"
```

**Step 2: Subgraph Retrieval**
```python
subgraph_id = "people.person.sibling_s|people.person.name"
subgraph = subgraph_index[subgraph_id]

# Retrieved subgraph:
{
    "relations": ["people.person.sibling_s", "people.person.name"],
    "pattern": "person → sibling → name",
    "example_triples": [
        ("m.0justin_bieber", "people.person.sibling_s", "m.0jaxon_bieber"),
        ("m.0jaxon_bieber", "people.person.name", "Jaxon Bieber")
    ],
    "answer_entities": ["m.0jaxon_bieber"]
}
```

**Step 3: Reader Model**
```python
reader_input = {
    "question": "What is the name of Justin Bieber's brother?",
    "subgraph": subgraph["example_triples"],
    "context": "Justin Bieber has a sibling. The sibling's name is Jaxon Bieber."
}

answer = reader_model.generate(reader_input)
# Output: "Jaxon Bieber"
```

---

## Why This Design?

1. **Efficiency**: T5-small can generate short relation sequences much faster than full paths
2. **Generalization**: Same relation pattern works for many questions
   - "What is X's brother's name?" → same pattern
   - "What is X's sister's name?" → same pattern
3. **Scalability**: Pre-computed subgraph index allows fast retrieval
4. **Modularity**: Can swap different reader models without retraining retriever

---

## Comparison Table: Subgraph ID vs. Direct Path

| Aspect | GSR (Subgraph ID) | Our Approach (Direct Path) |
|--------|-------------------|---------------------------|
| **Output** | `"people.person.sibling_s\|people.person.name"` | `[e1, r1, e2, r2, e3]` |
| **Length** | Short string (relation names) | Long sequence (entity+relation indices) |
| **Flexibility** | Limited to pre-indexed patterns | Any valid path in graph |
| **Speed** | Fast (short generation) | Slower (longer sequence) |
| **Index Required** | Yes (pre-computed) | No (on-the-fly) |
| **Model Size** | Small (T5-small/base) | Larger (custom architecture) |

